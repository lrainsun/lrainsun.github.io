---
layout: post
title:  "SLO告警"
date:   2019-10-30
categories: Others
tags: SLO
excerpt: Google SRE - SLO告警
mathjax: true

---

# SLI/SLO的定义

最近在看SLO，为啥要定义SLO？摘抄自Google SRE Book：

* What is SLI/SLO?

  SLO — Service level objectives (SLOs) specify a target level for the reliability of your service

  SLI — an SLI (service level indicators) is an indicator of the level of service that you are providing

  Error Budget — the SLO is a target percentage and the error budget is 100% minus the SLO

* Why we need SLO?

  key to making data-driven decisions about reliability

  determine what engineering work to prioritize

  key performance or reporting metric

下面是一些理解，不一定正确，是最近学习的一个结果。

SLI是一个指标，这个指标一定是比较关键的，可以代表系统提供的服务的水平的，我们讨论下来：

* SLO要么是用户关心的，这个指标如果不满足customer会立马感知，而且会unhappy，比如vm不通了
* 要么是admin关心的，用户虽然看不到，但是这些指标如果不满足，会影响到用户体验，或者在不久的将来会影响用户，比如capacity不足了，很有可能用户起vm就起不起来了
* 通常包含有：latency, error rate等，关于throughput应不应该放进去，我们还在争执中（个人觉得throughput是我们不能控制的，不能用以衡量service）
* SLI一定不是越多越好，要监控真正有用的指标，才能真正起到作用

SLO呢是根据SLI对应的，评价了service的可靠性。我们认为SLI跟SLO的关系是以一一对应的。

* 好的SLO可以帮助我们做决策（从那个方面可以提高service reliability）
* 还可以指导engineer应该把哪些工作放在更高优先级
* SLO可以是分层的，比如在throughput比较低的情况下，latency的要求应该高一些，而在throughput比较高的情况下，latency的要求可以适当放宽

初步我们准备用一个统一的公式来定义SLI，所有的事件都可以被划分为good events or bad events

那么SLI就有一个统一的定义：

```
( good events / total events ) * 100%
```

而SLO就可以定义为：

```
( good events / total events ) * 100% >= 99%  in last month 
```

而关于SLO跟error budget之间的关系，我们还没有达成定论和共识。

比如当我们定义这样一个SLI/SLO的时候：

SLI：Error rate of creating servers over last 1 hour <= 10%

SLO: The percentage for ”Error rate of creating servers over last 1 day <= 10%“ for last month > 90%

那这个时候我们要怎么计算error budget呢？有两个问题：

1. 月初的时候，剩余的error budget应该定义为多少?
2. 当某一小时error rate > 10%了之后，我们要从error budget里扣掉什么？怎么扣？

至今仍然晕晕的，所以看下Alerting on SLOs，希望能找到些答案

# Alerting Considerations

Alerting应该也是根据SLO/SLI定义出来的，目的呢是在consume too much error budget之前，给出警示，赶紧介入处理，SLO的alert应该作为最高优先级来处理。

要定义alert，就需要把SLI和error budget综合起来考虑，制定一个特定的规则。我们想要被通知到的情况是：一个事件消耗了大量的error budget，就需要引起重视。定义alert的时候需要考虑以下一些strategy：

* Precision（精确度）

  alert检测到significant event的比例，如果每个alert都对应一个significant event，那么精确度就是100%的。在低流量的情况下，alert对于nonsignificant events会变得异常敏感

* Recall

  如果每个significant event都被检测（被告警）到了，那么recall就是100%的

* Detection time

  在不同的条件下需要多长时间发送通知，比较长的检测时间会对error budget产生负面的影响

* Reset time

  当一个issue被发现后，告警的触发会持续多长时间？如果时间太长可能会导致混乱或者issue被忽略

# Ways to Alert on Significant Events

Google SRE Book里定义了6种方法可以来定义alert的，为了达到以上的几点要求

”error budget"是可以允许的bad events的数目

“error rate"是可以允许的bad events跟total events的ratio

## 1. Target Error Rate >= SLO Threshold

一般情况下，可以选择一个small time window（比如10分钟），然后当这个window的error rate exceeds SLO的时候就触发告警

```
For example, if the SLO is 99.9% over 30 days, alert if the error rate over the previous 10 minutes is ≥ 0.1%
```

这种当最近的时间窗超出SLO定义即告警的方式意味着系统发现了error budget的消耗：

```
alerting window size / reporting period
```

比如30天是一个reporting period，alerting window size是10分钟，那么当这十分钟error rate > 0.1%了，就扣除掉  （10 * 0.1%） / (30 * 24 * 60 * 0.1%) 也就是*0.02%*的error budget

30 * 24 * 60 * 0.1% -> 一个月30天，error budget 0.1%，总共43.2分钟

10 * 0.1% -> 10分钟的window，0.1%的error rate，也就是扣掉0.01分钟

![#detection-time](https://lh3.googleusercontent.com/mpK6cdpAGND9oCGaTETVhOmXF5TZegWCspB5h4p2JJmWmAgOgdEE8LVMSod2YSU5JGlMs0Ok7rKz7LvRwi48QrMRf0WTByDB4ynP=s0)

这张图是说，在各个error rate下到达alert阈值的detection time是多少。如果error rate很低只有1%，那我需要持续1min才发现，如果error rate非常高到10%的话，只需要几秒就报警了，10分钟内，如果error rate很快达到就很快告警

采用这种方式有以下一些好处和坏处：

Pros：

* detection time很及时
* 基本上每个event都被发现了，recall率很高

Cons：

* precision很低，很多没有威胁到SLO的event也触发了告警，每10分钟仅消耗0.02%每月的error budget
* 每天会收到非常多的告警

## 2. Increased Alert Window

在前面方法的基础上修改alert window的size来提高精确度，增加window size，会在触发告警时其实是消耗了更多的budget，比如我们希望在consume了5% error budget的时候收到告警，那么detection time就是

 ```
（ 1 - SLO）/ error ratio * alerting window size
 ```

![#error-rate-over-a-36-hour-period](https://lh3.googleusercontent.com/l5RR_VJmzwXDJfT-zuOj2YvfnRnSoOhGUzDvpQOE9Txq1fH3BUhJ1qHsw-dQBWOtoeIk6eTRu3QPyPFs9qOWVWCncFGu8ZwhxUvh=s0)

从这个图可以看出，虽然error rate在36小时内已经降到了可以忽略的水平，但是36小时平均的error rate仍然超出了threshold

这个方法的优劣有：

Pros:

* detection time仍然不错
* 精确度比上个例子中有所提高

Cons：

* reset time很差，2分钟后就开始出发告警，而且在之后的36小时内都一直在告警
* 因为时间比较长，数据量比较多，可能会对内存和I/O造成压力

## 3. Incrementing Alert Duration

可以增加一个alert duration的定义，在一段时间内如果value恢复到threshold以下，那么就不会触发告警，可以把这点加到上面的longer windows中

![#service_with_100percent_error_spikes_ever](https://lh3.googleusercontent.com/v1je8P1Sp24yIu1v3EJuy2aH47hCpZksaQ7q2zbK9yWEY8cKeExMYi0jSdvqxlUAIGzDfiQm0-EDUqDOpiIt8iknFKJgRFzvE-NhDg=s0)

图是5-min window of 10-min duration，从中可以看出有三个spike，大概consume了12%的30天的error budget，可是却不再会被持续告警

Pros：

* 精确度会比较高，检测到的必然是重要event

Cons：

* poor recall，不是所有重要event都会被检测到
* poor detection time，发现时间也比较长，因为告警持续时间并不能随着incident的严重程度scale，一个100%的outage跟一个0.02%的outage一样，可能都会在一小时后被告警
* 如果value总是立即恢复到threshold以下，duration timer会被重置，那么很可能永远不会告警

## 4. Alert on Burn Rate

为了改善前面的方法，可以增加一个burn rate

burn rate --- 对于SLO，消耗error budget的速率

![#error_budgets_relative_to_burn_rates](https://lh3.googleusercontent.com/XG34GssRWHMFGz0S5QjVsihHR444ZARuk9ynjq6fFCOmWwPrs7boARO7QncJ33JhTte9j6qGkj8Xag8Cz7i89kjxTLZZQ6q9ydOPRA=s0)

图表达了error budgets跟burn rates的关系，这里burn rate是1，也就是在end of the SLO time window，剩下0 budget。比如30天SLO 99.9%，一个0.1%的error rate使用了所有的error budget：burn rate是1

当alert window固定为1小时，决定5%的error budget消耗需要通知，那么一个burn rate-based alert应该这样定义：

```
( 1 - SLO) / error rate * alerting window size * burn rate
```

消耗的error budget是：

```
( burn rate * alerting window size) / period
```

Pros：

* 高精确度
* 短时间周期，容易计算
* detection time短
* reset time有提高，58分钟

Cons：

* low recall，不发现是每个event都会被发现
* 58分钟的reset time依然比较高

## 5. Multiple Burn Rate Alerts

告警逻辑可以使用多个burn rates和time windows，这样可以保证burn rate的优势，而且不需要去关注lower but significant error rates

对不会引起注意的事件set up ticket也是一个很好的主意，因为有足够的时间去解决

我们推荐一小时内消耗2%的budget，以及6小时内消耗5%的budget for paging

3天消耗10%的budget for ticket alerts

![#recommended_time_windows_and_burn_rates_f](https://lh3.googleusercontent.com/SLw760xP6lIoejFO8BA7L3QhNg6IohTR0a8qzX8NnrUyxqtReRPOrLm5NEPK1VhU14tMUK5chuKvPGDAdnGwnwsw4fjUGH1AwAm5ew=s0)

根据error rate的dection time, alert notification

Pros：

* 根据严重程度设置告警，error rate很高迅速告警，在最终告警如果error rate很低但是持续
* 精确度很高
* good recall，因为有3天window
* 可以根据具体defend SLO需要多快来定义告警类型

Cons：

* 需要管理更多的数据，window size，thresholds，reason
* reset time甚至更长，比如3天window
* 为了防止重复告警（满足多个条件），需要告警抑制

## 6. Multiwindow, Multi-Burn-Rate Alerts

我们可以enhance multi-burn-rate，需要添加一个参数，一个shorter window用来检查当我们触发告警的时候error budget是不是仍然在被消耗

一个推荐是，shorter window定义为long window的1/12的时长

![#short_and_long_windows_for_alerting](https://lh3.googleusercontent.com/zXV7tBjExfvf6CKjs_v4orEQFED7kEJHq7bS6TcOhogGFcjDNTrlUhyp2L0fOE8IuASrBQ9yvE6-vs6Rc8Upzr15RGfYKwcIvcAnkw=s0)

short & long widows for alerting

Pros：

* 灵活的告警框架，可以根据事件的严重程度和组织的要求来控制告警的类型
* 精确度很高
* good recall，因为3天window

Cons：

* 许多参数需要定义，alerting rule比较复杂，难以管理

可是看完还是有点云里雾里😥，最近team里也一直在讨论，会再更新