---
layout: post
title:  "SLO告警"
date:   2019-10-31 17:30:00 +0800
categories: Others
tags: SLO
excerpt: Google SRE - SLO告警
mathjax: true
typora-root-url: ../
---

# SLI/SLO的定义

最近在看SLO，为啥要定义SLO？摘抄自Google SRE Book：

* What is SLI/SLO?

  SLO — Service level objectives (SLOs) specify a target level for the reliability of your service

  SLI — an SLI (service level indicators) is an indicator of the level of service that you are providing

  Error Budget — the SLO is a target percentage and the error budget is 100% minus the SLO

* Why we need SLO?

  key to making data-driven decisions about reliability

  determine what engineering work to prioritize

  key performance or reporting metric

下面是一些理解，不一定正确，是最近学习的一个结果。

SLI是一个指标，这个指标一定是比较关键的，可以代表系统提供的服务的水平的，我们讨论下来：

* SLO要么是用户关心的，这个指标如果不满足customer会立马感知，而且会unhappy，比如vm不通了
* 要么是admin关心的，用户虽然看不到，但是这些指标如果不满足，会影响到用户体验，或者在不久的将来会影响用户，比如capacity不足了，很有可能用户起vm就起不起来了
* 通常包含有：latency, error rate等，关于throughput应不应该放进去，我们还在争执中（个人觉得throughput是我们不能控制的，不能用以衡量service）
* SLI一定不是越多越好，要监控真正有用的指标，才能真正起到作用

SLO呢是根据SLI对应的，评价了service的可靠性。我们认为SLI跟SLO的关系是一一对应的：

* 好的SLO可以帮助我们做决策（从那个方面可以提高service reliability）
* 还可以指导engineer应该把哪些工作放在更高优先级
* SLO可以是分层的，比如在throughput比较低的情况下，latency的要求应该高一些，而在throughput比较高的情况下，latency的要求可以适当放宽

初步我们准备用一个统一的公式来定义SLI，所有的事件都可以被划分为good events or bad events

那么SLI就有一个统一的定义：

```
( good events / total events ) * 100%
```

而SLO就可以定义为：

```
( good events / total events ) * 100% >= 99%  in last month 
```

而关于SLO跟error budget之间的关系，我们还没有达成定论和共识。

比如当我们定义这样一个SLI/SLO的时候：

SLI：Error rate of creating servers over last 1 hour <= 10%

SLO: The percentage for ”Error rate of creating servers over last 1 day <= 10%“ for last month > 90%

那这个时候我们要怎么计算error budget呢？有两个问题：

1. 月初的时候，剩余的error budget应该定义为多少?
2. 当某一小时error rate > 10%了之后，我们要从error budget里扣掉什么？怎么扣？

至今仍然晕晕的，所以看下Alerting on SLOs，希望能找到些答案

# Alerting Considerations

Alerting应该也是根据SLO/SLI定义出来的，目的呢是在consume too much error budget之前，给出警示，赶紧介入处理，SLO的alert应该作为最高优先级来处理。

要定义alert，就需要把SLI和error budget综合起来考虑，制定一个特定的规则。我们想要被通知到的情况是：一个事件消耗了大量的error budget，就需要引起重视。定义alert的时候需要考虑以下一些strategy：

* Precision（精确度）

  alert检测到significant event的比例，如果每个alert都对应一个significant event，那么精确度就是100%的。在低流量的情况下，alert对于nonsignificant events会变得异常敏感

* Recall

  如果每个significant event都被检测（被告警）到了，那么recall就是100%的

* Detection time

  在不同的条件下需要多长时间发送通知，比较长的检测时间会对error budget产生负面的影响

* Reset time

  当一个issue被发现后，告警的触发会持续多长时间？如果时间太长可能会导致混乱或者issue被忽略

# Ways to Alert on Significant Events

Google SRE Book里定义了6种方法可以来定义alert的，为了达到以上的几点要求

”error budget"是可以允许的bad events的数目

“error rate"是可以允许的bad events跟total events的ratio

## 1. Target Error Rate >= SLO Threshold

一般情况下，可以选择一个small time window（比如10分钟），然后当这个window的error rate exceeds SLO的时候就触发告警

```
For example, if the SLO is 99.9% over 30 days, alert if the error rate over the previous 10 minutes is ≥ 0.1%
```

这种当最近的时间窗超出SLO定义即告警的方式意味着系统发现了error budget的消耗：

```
alerting window size / reporting period
```

比如30天是一个reporting period，alerting window size是10分钟，那么当这十分钟error rate > 0.1%了，就扣除掉  （10 * 0.1%） / (30 * 24 * 60 * 0.1%) 也就是*0.02%*的error budget

30 * 24 * 60 * 0.1% -> 一个月30天，error budget 0.1%，总共43.2分钟

10 * 0.1% -> 10分钟的window，0.1%的error rate，也就是扣掉0.01分钟，也就是0.6秒

![mpK6cdpAGND9oCGaTETVhOmXF5TZegWCspB5h4p2JJmWmAgOgdEE8LVMSod2YSU5JGlMs0Ok7rKz7LvRwi48QrMRf0WTByDB4ynP=s0](/../assets/images/mpK6cdpAGND9oCGaTETVhOmXF5TZegWCspB5h4p2JJmWmAgOgdEE8LVMSod2YSU5JGlMs0Ok7rKz7LvRwi48QrMRf0WTByDB4ynP=s0.png)

这张图是说，在各个error rate下到达alert阈值的detection time是多少。如果error rate很低只有1%，那我需要持续1min才发现，如果error Arate非常高到10%的话，只需要几秒就报警了，10分钟内，如果error rate很快达到就很快告警

采用这种方式有以下一些好处和坏处：

Pros：

* detection time很及时，每0.6秒就可以发现一个event
* 基本上每个event都被发现了，recall率很高

Cons：

* precision很低，很多没有威胁到SLO的event也触发了告警，每10分钟仅消耗0.02%每月的error budget
* 每天会收到非常多的告警

## 2. Increased Alert Window

在前面方法的基础上修改alert window的size来提高精确度，增加window size，会在触发告警时其实是消耗了更多的budget，比如我们希望在consume了5% error budget的时候收到告警，那么detection time就是

 ```
（ 1 - SLO）/ error ratio * alerting window size
 ```

![#error-rate-over-a-36-hour-period](/assets/images/l5RR_VJmzwXDJfT-zuOj2YvfnRnSoOhGUzDvpQOE9Txq1fH3BUhJ1qHsw-dQBWOtoeIk6eTRu3QPyPFs9qOWVWCncFGu8ZwhxUvh=s0.png)

从这个图可以看出，虽然error rate在36小时内已经降到了可以忽略的水平，但是36小时平均的error rate仍然超出了threshold

这个方法的优劣有：

Pros:

* detection time仍然不错
* 精确度比上个例子中有所提高

Cons：

* reset time很差，2分钟后就开始出发告警，而且在之后的36小时内都一直在告警
* 因为时间比较长，数据量比较多，可能会对内存和I/O造成压力

## 3. Incrementing Alert Duration

可以增加一个alert duration的定义，在一段时间内如果value恢复到threshold以下，那么就不会触发告警，可以把这点加到上面的longer windows中

![#service_with_100percent_error_spikes_ever](/../assets/images/v1je8P1Sp24yIu1v3EJuy2aH47hCpZksaQ7q2zbK9yWEY8cKeExMYi0jSdvqxlUAIGzDfiQm0-EDUqDOpiIt8iknFKJgRFzvE-NhDg=s0.png)

图是5-min window of 10-min duration，从中可以看出有三个spike，大概consume了12%的30天的error budget，可是却不再会被持续告警

Pros：

* 精确度会比较高，检测到的必然是重要event

Cons：

* poor recall，不是所有重要event都会被检测到
* poor detection time，发现时间也比较长，因为告警持续时间并不能随着incident的严重程度scale，一个100%的outage跟一个0.02%的outage一样，可能都会在一小时后被告警
* 如果value总是立即恢复到threshold以下，duration timer会被重置，那么很可能永远不会告警

## 4. Alert on Burn Rate

为了改善前面的方法，可以增加一个burn rate

burn rate --- 对于SLO，消耗error budget的速率

![#error_budgets_relative_to_burn_rates](/../assets/images/XG34GssRWHMFGz0S5QjVsihHR444ZARuk9ynjq6fFCOmWwPrs7boARO7QncJ33JhTte9j6qGkj8Xag8Cz7i89kjxTLZZQ6q9ydOPRA=s0.png)

图表达了error budgets跟burn rates的关系，这里burn rate是1，也就是在end of the SLO time window，剩下0 budget。比如30天SLO 99.9%，一个0.1%的error rate使用了所有的error budget：burn rate是1

当alert window固定为1小时，决定5%的error budget消耗需要通知，那么一个burn rate-based alert应该这样定义：

```
( 1 - SLO) / error rate * alerting window size * burn rate
```

消耗的error budget是：

```
( burn rate * alerting window size) / period
```

Pros：

* 高精确度
* 短时间周期，容易计算
* detection time短
* reset time有提高，58分钟

Cons：

* low recall，不发现是每个event都会被发现
* 58分钟的reset time依然比较高

## 5. Multiple Burn Rate Alerts

告警逻辑可以使用多个burn rates和time windows，这样可以保证burn rate的优势，而且不需要去关注lower but significant error rates

对不会引起注意的事件set up ticket也是一个很好的主意，因为有足够的时间去解决

我们推荐一小时内消耗2%的budget，以及6小时内消耗5%的budget for paging

3天消耗10%的budget for ticket alerts

![#recommended_time_windows_and_burn_rates_f](/assets/images/SLw760xP6lIoejFO8BA7L3QhNg6IohTR0a8qzX8NnrUyxqtReRPOrLm5NEPK1VhU14tMUK5chuKvPGDAdnGwnwsw4fjUGH1AwAm5ew=s0.png)

根据error rate的dection time, alert notification

Pros：

* 根据严重程度设置告警，error rate很高迅速告警，在最终告警如果error rate很低但是持续
* 精确度很高
* good recall，因为有3天window
* 可以根据具体defend SLO需要多快来定义告警类型

Cons：

* 需要管理更多的数据，window size，thresholds，reason
* reset time甚至更长，比如3天window
* 为了防止重复告警（满足多个条件），需要告警抑制

## 6. Multiwindow, Multi-Burn-Rate Alerts

我们可以enhance multi-burn-rate，需要添加一个参数，一个shorter window用来检查当我们触发告警的时候error budget是不是仍然在被消耗

一个推荐是，shorter window定义为long window的1/12的时长

![#short_and_long_windows_for_alerting](/../assets/images/zXV7tBjExfvf6CKjs_v4orEQFED7kEJHq7bS6TcOhogGFcjDNTrlUhyp2L0fOE8IuASrBQ9yvE6-vs6Rc8Upzr15RGfYKwcIvcAnkw=s0.png)

short & long widows for alerting

Pros：

* 灵活的告警框架，可以根据事件的严重程度和组织的要求来控制告警的类型
* 精确度很高
* good recall，因为3天window

Cons：

* 许多参数需要定义，alerting rule比较复杂，难以管理

可是看完还是有点云里雾里😥，最近team里也一直在讨论，会再更新

---

>  10.31 update

# SLO & Error Budget

## SLO跟Error Budget的关系

之前上面的疑问，现在有个解答是这样的：

```
SLI：Error rate of creating servers over last 1 hour <= 10%
```

这里`1 hour`是一个统计的time window

```
SLO: The percentage for ”Error rate of creating servers over last 1 hour <= 10%“ for last month > 99.9 %
```

这里`1 month`是reporting period，90%是SLO

当我们知道SLO是`99.9%`之后，就可以把99.9%换算成时间来计算剩余的Error Budget

1个月我们假设按30天来计算，那么一个月里一共多少分钟呢？30 * 24 * 60 = `43200` 分钟

而我们要求的SLO是99.9%，反过来，也就说允许0.1%的Error。那么我们按照这个比例把一个月的Error Budget算出来，也就是一个月的总分钟数 * 0.1%，43200 * 0.1% = `43.2`分钟

## 扣除Error Budget

那么当error发生的时候，要怎么从error budget里扣时间呢？

这个跟统计的time window有关，比如time window是1 hour

考虑两个场景：

1. 在某一个time window里，总共只有用户boot了1台vm，失败了，也就是error rate是100%

   那么这个时候需要扣除的error budget是这一个整个time window时长的100%，60 * 100% = 60分钟，error budget就要扣掉60分钟

   妈呀，一下子SLO就用完了，所以这个time window设置成多少也是很有讲究的，对于boot vm这类不持续性的随机性的事件，设置long time window风险是会比较大的

2.  在某一个time window里，总共用户boot了1000台vm，失败了1台，也就是error rate是1/1000 = 0.1%

   这个时候需要扣除的error budget是time window时长的0.1%，也就是60 * 0.1% = 0.06分钟

   当event总数比较多的时候，每一个error的影响面显然就小多了

# SLO分层

SLO应该分为不同的层级

* admin level
* tenant level
* instance level

## User Related

下午开会讨论纠结的一个问题点在于，对于网卡丢包率<=1%这样一个SLO，有以下两种说法：

1. 如果有100个网卡，其中一个网卡丢包率>1%了，那么在这个时间段，可以认为网卡的服务是不available的，所以应该，从该网卡丢包率>1%的那个时间点开始计时，比如30s丢包率>1%，error budget就应该扣掉30s
2. 如果有100个网卡，其中一个网卡丢包率>1%了，time window是1分钟，虽然有一个网卡丢包率超标了，但是有99个网卡是好的，所以error budget应该扣除1/100 * 1分钟

其实两个说法是理解的角度不一样：

1. 对某一个instance来说，假如说有多个SLO指标，比如sg是否生效，以及网卡是否丢包，假如在过去一分钟，sg是完全正常的，可是网卡丢包率非常严重，那么我们也认为这个instance在这一分钟内是不可以用的，error budget扣掉一分钟。好像你买衣服，破了一个洞，你不能说我这领子是好的，衣服就没问题，我得把这件衣服的钱退给你。
2. 对某个tenant来讲，有多个vm，其中一个vm不可用，其他99个vm都可用，我们只承认1%的不可用。好像你买十件衣服，一件衣服坏了，我只能退给你一件衣服的钱，不可能把十件衣服的钱都退给你。

所以，讨论下来，SLO应该要增加instance level的视图

## Admin Related

另外对于admin，我们纠结，capacity相关的内容是否应该放进SLO

比如我这个aggregate的resource用完了，虽然用完了，可是如果没有tenant再来boot vm，那么这个情况就是ok的，并没有发生真正的故障，这样的指标需要记到SLO里吗？

我们认为，admin level的SLO并不公开，是给admin自己看的，加上这样的指标，可以给自己一个警示。所以暂时我们还是加上了这部分内容。



# References

[1] [http://landing.google.com/sre/workbook/chapters/alerting-on-slos/](http://landing.google.com/sre/workbook/chapters/alerting-on-slos/)